import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import sys
from ..core.rag import RAG

sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))
from config import settings
import warnings
import logging

# è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ‘åˆ¶
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
os.environ["TOKENIZERS_PARALLELISM"] = "false"


class LLM:
    def __init__(self, model_name):
        # settings.pyã‹ã‚‰GPUè¨­å®šã‚’å–å¾—
        use_gpu = settings.USE_GPU and torch.cuda.is_available()

        # GPUãŒå¼·åˆ¶ã•ã‚Œã¦ã„ã‚‹ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if settings.FORCE_GPU and not torch.cuda.is_available():
            raise RuntimeError("GPU is forced but CUDA is not available")

        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        if use_gpu:
            device = "cuda"
            # GPU_DEVICEè¨­å®šã‚’ä½¿ç”¨
            if settings.GPU_DEVICE == "auto":
                device_map = "auto"
            else:
                device_map = settings.GPU_DEVICE
        else:
            device = "cpu"
            device_map = None

        # ãƒ‡ãƒ¼ã‚¿å‹è¨­å®š
        if settings.TORCH_DTYPE == "auto":
            torch_dtype = torch.float16 if use_gpu else torch.float32
        elif settings.TORCH_DTYPE == "float16":
            torch_dtype = torch.float16
        elif settings.TORCH_DTYPE == "float32":
            torch_dtype = torch.float32
        elif settings.TORCH_DTYPE == "bfloat16":
            torch_dtype = torch.bfloat16
        else:
            torch_dtype = torch.float32  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        # é‡å­åŒ–è¨­å®š
        load_in_8bit = settings.QUANTIZATION == "8bit"
        load_in_4bit = settings.QUANTIZATION == "4bit"

        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰è¨­å®šã‚’æ§‹ç¯‰
        model_kwargs = {
            "torch_dtype": torch_dtype,
            "device_map": device_map,
        }

        if load_in_8bit:
            model_kwargs["load_in_8bit"] = True
        elif load_in_4bit:
            model_kwargs["load_in_4bit"] = True

        self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)

        # æ‰‹å‹•ã§ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•ã™ã‚‹å ´åˆï¼ˆdevice_mapã‚’ä½¿ã‚ãªã„å ´åˆï¼‰
        if device_map is None and use_gpu:
            self.model = self.model.to(device)

        self.model.eval()
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        # GPUä½¿ç”¨çŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›
        print(f"ğŸ”§ GPUè¨­å®š:")
        print(f"   CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   GPUæ•°: {torch.cuda.device_count()}")
            print(f"   ç¾åœ¨ã®GPU: {torch.cuda.current_device()}")
            print(f"   GPUå: {torch.cuda.get_device_name()}")
        print(f"   USE_GPUè¨­å®š: {settings.USE_GPU}")
        print(f"   å®Ÿéš›ã«GPUä½¿ç”¨: {use_gpu}")
        print(f"   ãƒ‡ãƒã‚¤ã‚¹ãƒãƒƒãƒ—: {device_map}")
        print(f"   ãƒ‡ãƒ¼ã‚¿å‹: {torch_dtype}")
        print(f"   ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹: {next(self.model.parameters()).device}")

        # pad_tokenã‚’è¨­å®šã—ã¦è­¦å‘Šã‚’æŠ‘åˆ¶
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.default_system_prompt = settings.DEFAULT_SYSTEM_PROMPT
        self.custom_system_prompt = settings.CUSTOM_SYSTEM_PROMPT
        self.max_tokens = settings.DEFAULT_MAX_TOKENS

    def process(self, messages):
        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        token_ids = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors="pt")

        with torch.no_grad():
            output_ids = self.model.generate(
                token_ids.to(self.model.device),
                max_new_tokens=self.max_tokens,
                do_sample=True,
                temperature=0.6,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id,
                attention_mask=torch.ones_like(token_ids),
            )
        output = self.tokenizer.decode(output_ids.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)
        return output

    def print_info(self):
        print(f"Model: {self.model}")
        # print(f"Tokenizer: {self.tokenizer}")

    def input_text(self, text):
        messages = [{"role": "system", "content": self.default_system_prompt}]
        if self.custom_system_prompt:
            messages.append({"role": "user", "content": self.custom_system_prompt})
        messages.append({"role": "user", "content": text})
        return self.process(messages)

    def input_text_list(self, texts) -> list:
        return [self.input_text(text) for text in texts]

    def summarize_with_context(self, user_request, context_data):
        """æ–‡è„ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦è¦ç´„ç”Ÿæˆ"""
        # æ–‡è„ˆãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        if isinstance(context_data, list):
            context_text = "\n".join([str(item.get("text", item)) for item in context_data])
        else:
            context_text = str(context_data)

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¦ç´„ç”¨ã«èª¿æ•´
        summary_system_prompt = """ã‚ãªãŸã¯å„ªç§€ãªãƒ­ã‚°åˆ†æãƒ»è¦ç´„ã®å°‚é–€å®¶ã§ã™ã€‚
æä¾›ã•ã‚ŒãŸãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«å¿œã˜ã¦é©åˆ‡ãªè¦ç´„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã‚¨ãƒ©ãƒ¼ã‚„å•é¡ŒãŒã‚ã‚‹å ´åˆã¯é‡è¦åº¦ã‚’ç¤ºã—ã€æ™‚ç³»åˆ—ã‚„åŸå› åˆ†æã‚‚å«ã‚ã¦ãã ã•ã„ã€‚
å›ç­”ã¯æ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚"""

        messages = [
            {"role": "system", "content": summary_system_prompt},
            {
                "role": "user",
                "content": f"ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚: {user_request}\n\nãƒ‡ãƒ¼ã‚¿:\n{context_text}",
            },
        ]

        return self.process(messages)

    def input_text_and_vector(self, text, vector):
        messages = [{"role": "system", "content": self.default_system_prompt}]
        if self.custom_system_prompt:
            messages.append({"role": "user", "content": self.custom_system_prompt})
        messages.append({"role": "user", "content": text, "vector": vector})
        return self.process(messages)


if __name__ == "__main__":
    llm = LLM(model_name=settings.MODEL)
    # llm.print_info()
    print(llm.input_text("Hello, world!"))
    print(llm.input_text_list(["Hello, world!", "How are you?"]))
    print(llm.input_text("ã¬ã‚‹ã½"))
    print("Done")

    # RAGã®ãƒ†ã‚¹ãƒˆ
    rag = RAG()
    # ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦ã‹ã‚‰ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
    rag.add_text("Hello world greeting message")
    rag.add_text("Python programming language")
    rag.add_text("Machine learning and AI")

    query_results = rag.query("Hello, world!")
    print(f"RAG query results: {query_results}")
    print(llm.input_text_and_vector("Hello, world!", query_results))
